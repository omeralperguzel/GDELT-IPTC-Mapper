#!/usr/bin/env python3
"""
GDELT Theme â†’ IPTC Media Topics Semantic Mapping

Bu script GDELT temalarÄ±nÄ± IPTC Media Topics taksonomisine eÅŸler.
YaklaÅŸÄ±m:
1. IPTC Ã¼st dÃ¼zey konularÄ±nÄ± (17 kategori) embedding uzayÄ±na yerleÅŸtir
2. GDELT temalarÄ±nÄ± aynÄ± uzaya yerleÅŸtir  
3. Her GDELT temasÄ±nÄ± en yakÄ±n IPTC konusuna cosine similarity ile ata

Referanslar:
- Kuzman & LjubeÅ¡iÄ‡ (2024): IPTC top-level news classification
- Tarekegn (2024): GDELT event clustering with LLM embeddings
- IPTC NewsCodes: https://iptc.org/std/NewsCodes/treeview/mediatopic/mediatopic-en-GB.html

RTX 3050 optimized: all-MiniLM-L6-v2 (384-dim, ~80MB)
"""

import json
import argparse
from pathlib import Path
import pandas as pd
import numpy as np

# Optional imports
try:
    from sentence_transformers import SentenceTransformer
    import torch
    HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False
    print("âš ï¸  sentence-transformers not available - using fallback mode")

try:
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.decomposition import PCA
    from sklearn.cluster import AgglomerativeClustering
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False
    print("âš ï¸  scikit-learn not available")

# ============================================================================
# CONFIGURATION
# ============================================================================

# Sentence-Transformer model (RTX 3050 optimized)
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
EMBEDDING_DIM = 384

# Files
IPTC_FILE = "iptc_mediatopics.csv"
VARGO_FILE = "vargo_gdelt_themes_issues.csv"
GDELT_CSV = "bquxjob_645c6baa_19b43fe1bcd.csv"  # total_docs by theme

# Output
OUTPUT_JSON = "gdelt_iptc_mapping.json"
OUTPUT_CSV = "gdelt_themes_iptc.csv"


# ============================================================================
# DATA LOADING
# ============================================================================

def load_iptc_topics(data_dir: Path) -> pd.DataFrame:
    """Load IPTC Media Topics taxonomy."""
    iptc_file = data_dir / IPTC_FILE
    
    if not iptc_file.exists():
        raise FileNotFoundError(f"IPTC file not found: {iptc_file}")
    
    iptc = pd.read_csv(iptc_file)
    
    # Create text representation for embedding
    iptc["text_repr"] = iptc["label"] + " - " + iptc["definition"]
    
    print(f"âœ… Loaded {len(iptc)} IPTC topics ({len(iptc[iptc['level']==1])} top-level)")
    return iptc


def load_gdelt_themes(data_dir: Path) -> pd.DataFrame:
    """Load unique GDELT themes from BigQuery export."""
    csv_file = data_dir / GDELT_CSV
    
    if not csv_file.exists():
        raise FileNotFoundError(f"GDELT CSV not found: {csv_file}")
    
    df = pd.read_csv(csv_file)
    
    # Get unique themes
    themes = df[['theme_code']].drop_duplicates().reset_index(drop=True)
    
    print(f"âœ… Loaded {len(themes)} unique GDELT themes from {csv_file.name}")
    return themes


def load_vargo_mapping(data_dir: Path) -> pd.DataFrame:
    """Load Vargo theme-to-issue mapping for additional context."""
    vargo_file = data_dir / VARGO_FILE
    
    if not vargo_file.exists():
        print(f"âš ï¸  Vargo mapping not found: {vargo_file}")
        return pd.DataFrame()
    
    vargo = pd.read_csv(vargo_file)
    print(f"âœ… Loaded Vargo mapping with {len(vargo)} theme-issue pairs")
    return vargo


# ============================================================================
# TEXT REPRESENTATION
# ============================================================================

def build_theme_text(row, vargo_map: dict) -> str:
    """
    Build rich text representation for GDELT theme.
    Format: "THEME_CODE - issue_category - description"
    """
    theme = row['theme_code']
    parts = [theme]
    
    # Add Vargo issue if available
    if theme in vargo_map:
        info = vargo_map[theme]
        if pd.notna(info.get('issue_category')):
            parts.append(str(info['issue_category']))
        if pd.notna(info.get('description')):
            parts.append(str(info['description']))
    
    return " - ".join(parts)


def prepare_theme_texts(themes: pd.DataFrame, vargo: pd.DataFrame) -> pd.DataFrame:
    """Prepare text representations for all themes."""
    
    # Build Vargo lookup
    vargo_map = {}
    if not vargo.empty:
        for _, row in vargo.iterrows():
            vargo_map[row['theme_code']] = {
                'issue_category': row.get('issue_category'),
                'description': row.get('description')
            }
    
    # Build text for each theme
    themes['text_repr'] = themes.apply(lambda r: build_theme_text(r, vargo_map), axis=1)
    themes['issue_category'] = themes['theme_code'].map(
        lambda t: vargo_map.get(t, {}).get('issue_category')
    )
    
    print(f"ğŸ“ Built text representations for {len(themes)} themes")
    print(f"   Sample: {themes['text_repr'].iloc[0][:80]}...")
    
    return themes


# ============================================================================
# EMBEDDING GENERATION
# ============================================================================

def generate_embeddings(texts: list, model=None) -> np.ndarray:
    """
    Generate sentence embeddings using all-MiniLM-L6-v2.
    Falls back to random embeddings if model not available.
    """
    if HAS_SENTENCE_TRANSFORMERS and model is not None:
        print(f"ğŸ§  Encoding {len(texts)} texts with {MODEL_NAME}...")
        embeddings = model.encode(
            texts,
            batch_size=64,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        return embeddings
    else:
        print(f"âš ï¸  Using random embeddings (sentence-transformers not installed)")
        np.random.seed(42)
        return np.random.randn(len(texts), EMBEDDING_DIM).astype(np.float32)


def load_model():
    """Load sentence-transformer model."""
    if not HAS_SENTENCE_TRANSFORMERS:
        return None
    
    import torch
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ Loading model on {device}...")
    
    model = SentenceTransformer(MODEL_NAME, device=device)
    return model


# ============================================================================
# IPTC MAPPING (SUPERVISED APPROACH)
# ============================================================================

def map_themes_to_iptc(
    theme_embeddings: np.ndarray,
    iptc_embeddings: np.ndarray,
    themes: pd.DataFrame,
    iptc: pd.DataFrame,
    top_level_only: bool = True
) -> pd.DataFrame:
    """
    Map each GDELT theme to nearest IPTC topic using cosine similarity.
    
    Args:
        theme_embeddings: GDELT theme embeddings (N x D)
        iptc_embeddings: IPTC topic embeddings (M x D)
        themes: GDELT themes DataFrame
        iptc: IPTC topics DataFrame
        top_level_only: If True, map only to top-level (17) IPTC categories
    
    Returns:
        themes DataFrame with IPTC mapping columns added
    """
    if not HAS_SKLEARN:
        print("âš ï¸  scikit-learn not available for similarity computation")
        themes['iptc_id'] = 'unknown'
        themes['iptc_label'] = 'Unknown'
        themes['similarity'] = 0.0
        return themes
    
    # Filter to top-level IPTC topics if requested
    if top_level_only:
        iptc_subset = iptc[iptc['level'] == 1].reset_index(drop=True)
        iptc_emb_subset = iptc_embeddings[iptc['level'] == 1]
    else:
        iptc_subset = iptc.reset_index(drop=True)
        iptc_emb_subset = iptc_embeddings
    
    print(f"ğŸ¯ Mapping {len(themes)} themes to {len(iptc_subset)} IPTC topics...")
    
    # Compute cosine similarity matrix: themes Ã— IPTC
    sim_matrix = cosine_similarity(theme_embeddings, iptc_emb_subset)
    
    # Find nearest IPTC topic for each theme
    closest_idx = np.argmax(sim_matrix, axis=1)
    max_sim = np.max(sim_matrix, axis=1)
    
    # Add mapping to themes DataFrame
    themes['iptc_id'] = iptc_subset.iloc[closest_idx]['medtop_id'].values
    themes['iptc_label'] = iptc_subset.iloc[closest_idx]['label'].values
    themes['iptc_definition'] = iptc_subset.iloc[closest_idx]['definition'].values
    themes['similarity'] = max_sim
    
    # Also get second-best match for comparison
    sim_matrix_copy = sim_matrix.copy()
    for i, idx in enumerate(closest_idx):
        sim_matrix_copy[i, idx] = -1  # mask best match
    second_idx = np.argmax(sim_matrix_copy, axis=1)
    second_sim = np.max(sim_matrix_copy, axis=1)
    
    themes['iptc_label_2nd'] = iptc_subset.iloc[second_idx]['label'].values
    themes['similarity_2nd'] = second_sim
    
    # Confidence: difference between best and second-best
    themes['confidence'] = themes['similarity'] - themes['similarity_2nd']
    
    return themes


# ============================================================================
# CLUSTERING (UNSUPERVISED APPROACH - OPTIONAL)
# ============================================================================

def cluster_themes(
    theme_embeddings: np.ndarray,
    themes: pd.DataFrame,
    n_clusters: int = 8
) -> pd.DataFrame:
    """
    Optional: Cluster themes using agglomerative clustering.
    This provides an alternative grouping independent of IPTC.
    """
    if not HAS_SKLEARN:
        themes['cluster_id'] = 0
        return themes
    
    print(f"ğŸ”„ Clustering themes into {n_clusters} groups...")
    
    # PCA for dimensionality reduction
    n_components = min(20, len(themes) - 1, theme_embeddings.shape[1])
    pca = PCA(n_components=n_components, random_state=42)
    X_reduced = pca.fit_transform(theme_embeddings)
    
    # Agglomerative clustering
    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
    cluster_labels = clusterer.fit_predict(X_reduced)
    
    themes['cluster_id'] = cluster_labels
    
    # Print cluster distribution
    for i in range(n_clusters):
        count = (cluster_labels == i).sum()
        print(f"   Cluster {i}: {count} themes")
    
    return themes


# ============================================================================
# ANALYSIS & EXPORT
# ============================================================================

def analyze_mapping(themes: pd.DataFrame, iptc: pd.DataFrame) -> dict:
    """Analyze IPTC mapping results."""
    
    # Count themes per IPTC category
    iptc_counts = themes['iptc_label'].value_counts().to_dict()
    
    # Average similarity per IPTC category
    iptc_sim = themes.groupby('iptc_label')['similarity'].mean().to_dict()
    
    # Low confidence mappings (ambiguous)
    low_conf = themes[themes['confidence'] < 0.1]
    
    # Build summary
    summary = {
        'total_themes': len(themes),
        'iptc_categories_used': len(iptc_counts),
        'avg_similarity': float(themes['similarity'].mean()),
        'avg_confidence': float(themes['confidence'].mean()),
        'low_confidence_count': len(low_conf),
        'themes_per_iptc': iptc_counts,
        'avg_similarity_per_iptc': iptc_sim
    }
    
    return summary


def export_results(themes: pd.DataFrame, iptc: pd.DataFrame, output_dir: Path):
    """Export mapping results to JSON and CSV."""
    
    # Analyze mapping
    summary = analyze_mapping(themes, iptc)
    
    # Prepare IPTC category details
    iptc_top = iptc[iptc['level'] == 1].copy()
    iptc_details = {}
    
    for _, row in iptc_top.iterrows():
        label = row['label']
        matched = themes[themes['iptc_label'] == label]
        
        iptc_details[label] = {
            'medtop_id': row['medtop_id'],
            'definition': row['definition'],
            'theme_count': len(matched),
            'themes': matched['theme_code'].tolist(),
            'avg_similarity': float(matched['similarity'].mean()) if len(matched) > 0 else 0
        }
    
    # Convert NaN values to None for JSON serialization
    def clean_value(v):
        if pd.isna(v):
            return None
        if isinstance(v, (np.floating, float)):
            if np.isnan(v) or np.isinf(v):
                return None
            return float(v)
        if isinstance(v, (np.integer, int)):
            return int(v)
        return v
    
    # Prepare themes list
    themes_list = []
    for _, row in themes.iterrows():
        themes_list.append({
            'theme_code': row['theme_code'],
            'issue_category': clean_value(row.get('issue_category')),
            'iptc_id': row['iptc_id'],
            'iptc_label': row['iptc_label'],
            'iptc_definition': row['iptc_definition'],
            'similarity': clean_value(row['similarity']),
            'iptc_label_2nd': row['iptc_label_2nd'],
            'similarity_2nd': clean_value(row['similarity_2nd']),
            'confidence': clean_value(row['confidence']),
            'cluster_id': clean_value(row.get('cluster_id', 0))
        })
    
    # Build final JSON
    result = {
        'metadata': {
            'model': MODEL_NAME,
            'embedding_dim': EMBEDDING_DIM,
            'method': 'IPTC Media Topics cosine similarity mapping',
            'reference': 'IPTC NewsCodes 2025-10-10',
            'total_themes': summary['total_themes'],
            'iptc_categories': summary['iptc_categories_used']
        },
        'summary': summary,
        'iptc_categories': iptc_details,
        'themes': themes_list
    }
    
    # Export JSON
    json_file = output_dir / OUTPUT_JSON
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"âœ… Exported: {json_file}")
    
    # Export CSV
    csv_file = output_dir / OUTPUT_CSV
    themes.to_csv(csv_file, index=False)
    print(f"âœ… Exported: {csv_file}")
    
    return result


# ============================================================================
# MAIN PIPELINE
# ============================================================================

def run_iptc_mapping_pipeline(data_dir: Path = None) -> dict:
    """Run the complete IPTC mapping pipeline."""
    
    if data_dir is None:
        data_dir = Path(__file__).parent
    
    print("\n" + "=" * 70)
    print("ğŸ¯ GDELT Theme â†’ IPTC Media Topics Mapping Pipeline")
    print("=" * 70 + "\n")
    
    # Step 1: Load IPTC topics
    print("ğŸ“¥ Step 1: Loading IPTC Media Topics...")
    iptc = load_iptc_topics(data_dir)
    
    # Step 2: Load GDELT themes
    print("\nğŸ“¥ Step 2: Loading GDELT themes...")
    themes = load_gdelt_themes(data_dir)
    
    # Step 3: Load Vargo mapping for additional context
    print("\nğŸ”— Step 3: Loading Vargo issue mapping...")
    vargo = load_vargo_mapping(data_dir)
    
    # Step 4: Prepare text representations
    print("\nğŸ“ Step 4: Building text representations...")
    themes = prepare_theme_texts(themes, vargo)
    
    # Step 5: Load model and generate embeddings
    print("\nğŸ§  Step 5: Generating embeddings...")
    model = load_model()
    
    # IPTC embeddings
    iptc_texts = iptc['text_repr'].tolist()
    iptc_embeddings = generate_embeddings(iptc_texts, model)
    
    # Theme embeddings
    theme_texts = themes['text_repr'].tolist()
    theme_embeddings = generate_embeddings(theme_texts, model)
    
    # Step 6: Map themes to IPTC
    print("\nğŸ¯ Step 6: Mapping themes to IPTC categories...")
    themes = map_themes_to_iptc(
        theme_embeddings, iptc_embeddings, themes, iptc, top_level_only=True
    )
    
    # Step 7: Optional clustering
    print("\nğŸ“Š Step 7: Additional clustering...")
    n_clusters = min(8, len(themes))
    themes = cluster_themes(theme_embeddings, themes, n_clusters=n_clusters)
    
    # Step 8: Export results
    print("\nğŸ’¾ Step 8: Exporting results...")
    result = export_results(themes, iptc, data_dir)
    
    # Print summary
    print("\n" + "=" * 70)
    print("ğŸ“Š MAPPING RESULTS SUMMARY")
    print("=" * 70)
    
    summary = result['summary']
    print(f"\nğŸ“ˆ Total themes mapped: {summary['total_themes']}")
    print(f"ğŸ“ IPTC categories used: {summary['iptc_categories_used']}")
    print(f"ğŸ“ Average similarity: {summary['avg_similarity']:.3f}")
    print(f"ğŸ¯ Average confidence: {summary['avg_confidence']:.3f}")
    print(f"âš ï¸  Low confidence mappings: {summary['low_confidence_count']}")
    
    print("\nğŸ“Š Themes per IPTC Category:")
    for iptc_label, count in sorted(summary['themes_per_iptc'].items(), key=lambda x: -x[1]):
        avg_sim = summary['avg_similarity_per_iptc'].get(iptc_label, 0)
        print(f"   â€¢ {iptc_label}: {count} themes (avg sim: {avg_sim:.3f})")
    
    print("\nâœ… Pipeline complete!")
    
    return result


# ============================================================================
# CLI
# ============================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Map GDELT themes to IPTC Media Topics"
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        default=None,
        help="Directory containing data files"
    )
    
    args = parser.parse_args()
    
    data_dir = Path(args.data_dir) if args.data_dir else Path(__file__).parent
    
    results = run_iptc_mapping_pipeline(data_dir)
